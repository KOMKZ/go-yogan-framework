package kafka

import (
	"context"
	"fmt"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	"go.uber.org/zap"
)

// ConsumerRunnerConfig è¿è¡Œå™¨é…ç½®
type ConsumerRunnerConfig struct {
	// GroupID æ¶ˆè´¹è€…ç»„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ handler.Name()ï¼‰
	GroupID string

	// Workers å¹¶å‘æ¶ˆè´¹è€…æ•°é‡ï¼ˆé»˜è®¤ 1ï¼‰
	Workers int

	// OffsetInitial åˆå§‹ Offsetï¼š-1=Newest, -2=Oldestï¼ˆé»˜è®¤ -1ï¼‰
	OffsetInitial int64

	// AutoCommit æ˜¯å¦è‡ªåŠ¨æäº¤ï¼ˆé»˜è®¤ trueï¼‰
	AutoCommit bool

	// AutoCommitInterval è‡ªåŠ¨æäº¤é—´éš”ï¼ˆé»˜è®¤ 1sï¼‰
	AutoCommitInterval time.Duration

	// MaxProcessingTime å•æ¡æ¶ˆæ¯æœ€å¤§å¤„ç†æ—¶é—´ï¼ˆé»˜è®¤ 30sï¼‰
	MaxProcessingTime time.Duration

	// SessionTimeout ä¼šè¯è¶…æ—¶ï¼ˆé»˜è®¤ 10sï¼‰
	SessionTimeout time.Duration

	// HeartbeatInterval å¿ƒè·³é—´éš”ï¼ˆé»˜è®¤ 3sï¼‰
	HeartbeatInterval time.Duration
}

// applyDefaults åº”ç”¨é»˜è®¤å€¼
func (c *ConsumerRunnerConfig) applyDefaults(handlerName string) {
	if c.GroupID == "" {
		c.GroupID = handlerName + "-group"
	}
	if c.Workers <= 0 {
		c.Workers = 1
	}
	if c.OffsetInitial == 0 {
		c.OffsetInitial = -1 // Newest
	}
	if c.AutoCommitInterval == 0 {
		c.AutoCommitInterval = time.Second
	}
	if c.MaxProcessingTime == 0 {
		c.MaxProcessingTime = 30 * time.Second
	}
	if c.SessionTimeout == 0 {
		c.SessionTimeout = 10 * time.Second
	}
	if c.HeartbeatInterval == 0 {
		c.HeartbeatInterval = 3 * time.Second
	}
}

// ConsumerRunner æ¶ˆè´¹è€…è¿è¡Œå™¨
// å°è£…ä¿¡å·å¤„ç†ã€Worker ç®¡ç†ã€ç”Ÿå‘½å‘¨æœŸæ§åˆ¶
type ConsumerRunner struct {
	manager *Manager
	handler ConsumerHandler
	config  ConsumerRunnerConfig
	logger  *zap.Logger

	consumers []*ConsumerGroup
	wg        sync.WaitGroup
	cancel    context.CancelFunc
	mu        sync.RWMutex
	running   bool
}

// NewConsumerRunner åˆ›å»ºæ¶ˆè´¹è€…è¿è¡Œå™¨
func NewConsumerRunner(manager *Manager, handler ConsumerHandler, cfg ConsumerRunnerConfig) *ConsumerRunner {
	cfg.applyDefaults(handler.Name())

	return &ConsumerRunner{
		manager: manager,
		handler: handler,
		config:  cfg,
		logger:  manager.logger.With(zap.String("consumer", handler.Name())),
	}
}

// Run é˜»å¡è¿è¡Œï¼ˆå†…éƒ¨å¤„ç†ä¿¡å·ï¼‰
// ä¼šé˜»å¡ç›´åˆ°æ”¶åˆ° SIGINT/SIGTERM ä¿¡å·
func (r *ConsumerRunner) Run(ctx context.Context) error {
	// å¯åŠ¨æ¶ˆè´¹è€…
	if err := r.Start(ctx); err != nil {
		return err
	}

	// è®¾ç½®ä¿¡å·å¤„ç†
	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

	r.logger.Info("ğŸ“¡ æ¶ˆè´¹è€…è¿è¡Œä¸­ï¼Œç­‰å¾…æ¶ˆæ¯... (æŒ‰ Ctrl+C é€€å‡º)",
		zap.String("group_id", r.config.GroupID),
		zap.Strings("topics", r.handler.Topics()),
		zap.Int("workers", r.config.Workers))

	// ç­‰å¾…ä¿¡å·æˆ–ä¸Šä¸‹æ–‡å–æ¶ˆ
	select {
	case sig := <-sigCh:
		r.logger.Info("ğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å·", zap.String("signal", sig.String()))
	case <-ctx.Done():
		r.logger.Info("ğŸ›‘ ä¸Šä¸‹æ–‡å·²å–æ¶ˆ")
	}

	// åœæ­¢æ¶ˆè´¹è€…
	return r.Stop()
}

// Start éé˜»å¡å¯åŠ¨
func (r *ConsumerRunner) Start(ctx context.Context) error {
	r.mu.Lock()
	if r.running {
		r.mu.Unlock()
		return fmt.Errorf("consumer runner is already running")
	}
	r.running = true
	r.mu.Unlock()

	// åˆ›å»ºå¯å–æ¶ˆçš„ä¸Šä¸‹æ–‡
	runCtx, cancel := context.WithCancel(ctx)
	r.cancel = cancel

	// æ„å»ºæ¶ˆè´¹è€…é…ç½®
	consumerCfg := ConsumerConfig{
		GroupID:            r.config.GroupID,
		Topics:             r.handler.Topics(),
		OffsetInitial:      r.config.OffsetInitial,
		AutoCommit:         r.config.AutoCommit,
		AutoCommitInterval: r.config.AutoCommitInterval,
		MaxProcessingTime:  r.config.MaxProcessingTime,
		SessionTimeout:     r.config.SessionTimeout,
		HeartbeatInterval:  r.config.HeartbeatInterval,
	}

	// å¯åŠ¨å¤šä¸ª Worker
	r.consumers = make([]*ConsumerGroup, r.config.Workers)
	for i := 0; i < r.config.Workers; i++ {
		workerID := i + 1
		consumerName := fmt.Sprintf("%s-worker-%d", r.handler.Name(), workerID)

		consumer, err := r.manager.CreateConsumer(consumerName, consumerCfg)
		if err != nil {
			// æ¸…ç†å·²åˆ›å»ºçš„æ¶ˆè´¹è€…
			r.cleanupConsumers()
			return fmt.Errorf("create consumer %s failed: %w", consumerName, err)
		}

		r.consumers[i] = consumer
		r.wg.Add(1)

		go r.runWorker(runCtx, workerID, consumer)

		r.logger.Info("âœ… Worker å¯åŠ¨æˆåŠŸ",
			zap.Int("worker_id", workerID),
			zap.String("consumer", consumerName))
	}

	r.logger.Info("ğŸš€ æ¶ˆè´¹è€…è¿è¡Œå™¨å·²å¯åŠ¨",
		zap.String("name", r.handler.Name()),
		zap.String("group_id", r.config.GroupID),
		zap.Int("workers", r.config.Workers),
		zap.Strings("topics", r.handler.Topics()))

	return nil
}

// runWorker è¿è¡Œå•ä¸ª Worker
func (r *ConsumerRunner) runWorker(ctx context.Context, workerID int, consumer *ConsumerGroup) {
	defer r.wg.Done()

	// åŒ…è£… handlerï¼Œæ·»åŠ  workerID åˆ°æ—¥å¿—
	wrappedHandler := func(ctx context.Context, msg *ConsumedMessage) error {
		return r.handler.Handle(ctx, msg)
	}

	err := consumer.Start(ctx, wrappedHandler)
	if err != nil && err != context.Canceled {
		r.logger.Error("worker å¼‚å¸¸é€€å‡º",
			zap.Int("worker_id", workerID),
			zap.Error(err))
	}
}

// Stop ä¼˜é›…åœæ­¢
func (r *ConsumerRunner) Stop() error {
	r.mu.Lock()
	if !r.running {
		r.mu.Unlock()
		return nil
	}
	r.running = false
	r.mu.Unlock()

	r.logger.Info("ğŸ›‘ æ­£åœ¨åœæ­¢æ¶ˆè´¹è€…...")

	// å–æ¶ˆä¸Šä¸‹æ–‡
	if r.cancel != nil {
		r.cancel()
	}

	// åœæ­¢æ‰€æœ‰æ¶ˆè´¹è€…
	r.cleanupConsumers()

	// ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆ
	r.wg.Wait()

	r.logger.Info("âœ… æ¶ˆè´¹è€…è¿è¡Œå™¨å·²åœæ­¢", zap.String("name", r.handler.Name()))
	return nil
}

// cleanupConsumers æ¸…ç†æ‰€æœ‰æ¶ˆè´¹è€…
func (r *ConsumerRunner) cleanupConsumers() {
	for i, consumer := range r.consumers {
		if consumer != nil {
			if err := consumer.Stop(); err != nil {
				r.logger.Error("stop consumer failed",
					zap.Int("worker_id", i+1),
					zap.Error(err))
			}
		}
	}
}

// IsRunning æ£€æŸ¥æ˜¯å¦è¿è¡Œä¸­
func (r *ConsumerRunner) IsRunning() bool {
	r.mu.RLock()
	defer r.mu.RUnlock()
	return r.running
}

// GetConfig è·å–é…ç½®
func (r *ConsumerRunner) GetConfig() ConsumerRunnerConfig {
	return r.config
}
