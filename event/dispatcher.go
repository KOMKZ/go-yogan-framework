package event

import (
	"context"
	"errors"
	"sort"
	"sync"
	"sync/atomic"

	"github.com/KOMKZ/go-yogan-framework/logger"
	"github.com/panjf2000/ants/v2"
	"go.uber.org/zap"
)

// UnsubscribeFunc å–æ¶ˆè®¢é˜…å‡½æ•°
type UnsubscribeFunc func()

// Dispatcher äº‹ä»¶åˆ†å‘å™¨æ¥å£
type Dispatcher interface {
	// Subscribe è®¢é˜…äº‹ä»¶ï¼Œè¿”å›å–æ¶ˆè®¢é˜…å‡½æ•°
	Subscribe(eventName string, listener Listener, opts ...SubscribeOption) UnsubscribeFunc

	// Dispatch åˆ†å‘äº‹ä»¶
	// æ”¯æŒ DispatchOption æ§åˆ¶åˆ†å‘è¡Œä¸ºï¼š
	// - é»˜è®¤ï¼šå†…å­˜åŒæ­¥åˆ†å‘
	// - WithDispatchAsync()ï¼šå†…å­˜å¼‚æ­¥åˆ†å‘
	// - WithKafka(topic)ï¼šå‘é€åˆ° Kafka
	// - WithKafka(topic) + WithDispatchAsync()ï¼šå¼‚æ­¥å‘é€åˆ° Kafka
	Dispatch(ctx context.Context, event Event, opts ...DispatchOption) error

	// DispatchAsync å¼‚æ­¥åˆ†å‘äº‹ä»¶ï¼ˆå…¼å®¹æ—§ APIï¼‰
	// ç­‰ä»·äº Dispatch(ctx, event, WithDispatchAsync())
	DispatchAsync(ctx context.Context, event Event)

	// Use æ³¨å†Œå…¨å±€æ‹¦æˆªå™¨
	Use(interceptor Interceptor)
}

// dispatcher äº‹ä»¶åˆ†å‘å™¨å®ç°
type dispatcher struct {
	mu             sync.RWMutex
	listeners      map[string][]listenerEntry
	interceptors   []Interceptor
	nextID         uint64
	pool           *ants.Pool
	poolSize       int
	logger         *logger.CtxZapLogger
	closed         int32
	kafkaPublisher KafkaPublisher // Kafka å‘å¸ƒè€…ï¼ˆå¯é€‰ï¼‰
	router         *Router        // äº‹ä»¶è·¯ç”±å™¨ï¼ˆå¯é€‰ï¼‰
	setAllSync     bool
}

// NewDispatcher åˆ›å»ºäº‹ä»¶åˆ†å‘å™¨
func NewDispatcher(opts ...DispatcherOption) *dispatcher {
	d := &dispatcher{
		listeners: make(map[string][]listenerEntry),
		poolSize:  100,
		logger:    logger.GetLogger("yogan"),
	}

	for _, opt := range opts {
		opt(d)
	}

	// åˆ›å»ºåç¨‹æ± 
	var err error
	d.pool, err = ants.NewPool(d.poolSize)
	if err != nil {
		d.logger.Error("åˆ›å»ºåç¨‹æ± å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®", zap.Error(err))
		d.pool, _ = ants.NewPool(100)
	}

	return d
}

// Subscribe è®¢é˜…äº‹ä»¶
func (d *dispatcher) Subscribe(eventName string, listener Listener, opts ...SubscribeOption) UnsubscribeFunc {
	if eventName == "" || listener == nil {
		return func() {}
	}

	entry := listenerEntry{
		id:       atomic.AddUint64(&d.nextID, 1),
		listener: listener,
	}

	for _, opt := range opts {
		opt(&entry)
		if d.setAllSync {
			entry.async = false
		}
	}

	d.mu.Lock()
	d.listeners[eventName] = append(d.listeners[eventName], entry)
	// æŒ‰ä¼˜å…ˆçº§æ’åº
	sort.SliceStable(d.listeners[eventName], func(i, j int) bool {
		return d.listeners[eventName][i].priority < d.listeners[eventName][j].priority
	})
	d.mu.Unlock()

	// è¿”å›å–æ¶ˆè®¢é˜…å‡½æ•°
	return func() {
		d.unsubscribe(eventName, entry.id)
	}
}

// unsubscribe å–æ¶ˆè®¢é˜…
func (d *dispatcher) unsubscribe(eventName string, id uint64) {
	d.mu.Lock()
	defer d.mu.Unlock()

	entries := d.listeners[eventName]
	for i, e := range entries {
		if e.id == id {
			d.listeners[eventName] = append(entries[:i], entries[i+1:]...)
			return
		}
	}
}

// Use æ³¨å†Œå…¨å±€æ‹¦æˆªå™¨
func (d *dispatcher) Use(interceptor Interceptor) {
	d.mu.Lock()
	d.interceptors = append(d.interceptors, interceptor)
	d.mu.Unlock()
}

// Dispatch åˆ†å‘äº‹ä»¶
// ä¼˜å…ˆçº§ï¼šä»£ç é€‰é¡¹ > é…ç½®è·¯ç”± > é»˜è®¤(å†…å­˜)
func (d *dispatcher) Dispatch(ctx context.Context, event Event, opts ...DispatchOption) error {
	if event == nil {
		return nil
	}

	// è§£æé€‰é¡¹
	options := &dispatchOptions{}
	for _, opt := range opts {
		opt(options)
	}

	// å¦‚æœä»£ç æ²¡æœ‰æ˜ç¡®æŒ‡å®šé©±åŠ¨å™¨ï¼Œå°è¯•ä»è·¯ç”±é…ç½®è·å–
	if !options.driverExplicit && d.router != nil {
		if route := d.router.Match(event.Name()); route != nil {
			d.logger.DebugCtx(ctx, "ğŸ¯ äº‹ä»¶è·¯ç”±åŒ¹é…æˆåŠŸ",
				zap.String("event", event.Name()),
				zap.String("driver", route.Driver),
				zap.String("topic", route.Topic))
			options.driver = route.Driver
			if route.Driver == DriverKafka && options.topic == "" {
				options.topic = route.Topic
			}
		}
	}

	// åº”ç”¨é»˜è®¤å€¼
	options.applyDefaults()

	// æ ¹æ®é©±åŠ¨å™¨é€‰æ‹©åˆ†å‘æ–¹å¼
	switch options.driver {
	case DriverKafka:
		return d.dispatchToKafka(ctx, event, options)
	default:
		// setAllSync å¼ºåˆ¶åŒæ­¥åˆ†å‘ï¼ˆå¿½ç•¥ options.asyncï¼‰
		if options.async && !d.setAllSync {
			d.dispatchAsyncMemory(ctx, event)
			return nil
		}
		return d.dispatchMemory(ctx, event)
	}
}

// dispatchMemory å†…å­˜åŒæ­¥åˆ†å‘
func (d *dispatcher) dispatchMemory(ctx context.Context, event Event) error {
	// è·å–æ‹¦æˆªå™¨å’Œç›‘å¬å™¨çš„å‰¯æœ¬
	d.mu.RLock()
	interceptors := make([]Interceptor, len(d.interceptors))
	copy(interceptors, d.interceptors)
	entries := make([]listenerEntry, len(d.listeners[event.Name()]))
	copy(entries, d.listeners[event.Name()])
	d.mu.RUnlock()

	// æ„å»ºæ‰§è¡Œé“¾ï¼šæ‹¦æˆªå™¨ -> ç›‘å¬å™¨
	handler := d.buildHandlerChain(ctx, entries, interceptors)

	err := handler(ctx, event)

	// æ¸…ç†ä¸€æ¬¡æ€§ç›‘å¬å™¨
	d.cleanupOnceListeners(event.Name(), entries)

	// ErrStopPropagation ä¸è§†ä¸ºé”™è¯¯
	if errors.Is(err, ErrStopPropagation) {
		return nil
	}

	return err
}

// dispatchAsyncMemory å†…å­˜å¼‚æ­¥åˆ†å‘
func (d *dispatcher) dispatchAsyncMemory(ctx context.Context, event Event) {
	if atomic.LoadInt32(&d.closed) == 1 {
		return
	}

	// å¤åˆ¶ context ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆé¿å… context è¢«å–æ¶ˆï¼‰
	asyncCtx := context.Background()
	if traceID := ctx.Value("trace_id"); traceID != nil {
		asyncCtx = context.WithValue(asyncCtx, "trace_id", traceID)
	}

	eventName := event.Name()

	err := d.pool.Submit(func() {
		if err := d.dispatchMemory(asyncCtx, event); err != nil {
			d.logger.ErrorCtx(asyncCtx, "å¼‚æ­¥äº‹ä»¶å¤„ç†å¤±è´¥",
				zap.String("event", eventName),
				zap.Error(err))
		}
	})

	if err != nil {
		d.logger.ErrorCtx(ctx, "æäº¤å¼‚æ­¥ä»»åŠ¡å¤±è´¥",
			zap.String("event", eventName),
			zap.Error(err))
	}
}

// dispatchToKafka å‘é€åˆ° Kafka
func (d *dispatcher) dispatchToKafka(ctx context.Context, event Event, opts *dispatchOptions) error {
	if d.kafkaPublisher == nil {
		return ErrKafkaNotAvailable
	}

	if opts.topic == "" {
		return ErrKafkaTopicRequired
	}

	// è·å– traceID
	traceID := ""
	if v := ctx.Value("trace_id"); v != nil {
		if s, ok := v.(string); ok {
			traceID = s
		}
	}

	// åºåˆ—åŒ–äº‹ä»¶
	payload, err := SerializeEvent(event, traceID)
	if err != nil {
		return err
	}

	// ç¡®å®šæ¶ˆæ¯ Key
	key := opts.key
	if key == "" {
		key = event.Name()
	}

	// å¼‚æ­¥å‘é€
	if opts.async {
		go func() {
			if err := d.kafkaPublisher.PublishJSON(ctx, opts.topic, key, payload); err != nil {
				d.logger.ErrorCtx(ctx, "Kafka å¼‚æ­¥å‘é€å¤±è´¥",
					zap.String("event", event.Name()),
					zap.String("topic", opts.topic),
					zap.Error(err))
			}
		}()
		return nil
	}

	// åŒæ­¥å‘é€
	return d.kafkaPublisher.PublishJSON(ctx, opts.topic, key, payload)
}

// DispatchAsync å¼‚æ­¥åˆ†å‘äº‹ä»¶ï¼ˆå…¼å®¹æ—§ APIï¼‰
// ç­‰ä»·äº Dispatch(ctx, event, WithDispatchAsync())
func (d *dispatcher) DispatchAsync(ctx context.Context, event Event) {
	_ = d.Dispatch(ctx, event, WithDispatchAsync())
}

// buildHandlerChain æ„å»ºæ‰§è¡Œé“¾
func (d *dispatcher) buildHandlerChain(ctx context.Context, entries []listenerEntry, interceptors []Interceptor) Next {
	// æœ€å†…å±‚ï¼šæ‰§è¡Œç›‘å¬å™¨
	handler := func(ctx context.Context, event Event) error {
		return d.executeListeners(ctx, event, entries)
	}

	// ä»åå‘å‰åŒ…è£…æ‹¦æˆªå™¨
	for i := len(interceptors) - 1; i >= 0; i-- {
		interceptor := interceptors[i]
		next := handler
		handler = func(ctx context.Context, event Event) error {
			return interceptor(ctx, event, next)
		}
	}

	return handler
}

// executeListeners æ‰§è¡Œç›‘å¬å™¨
func (d *dispatcher) executeListeners(ctx context.Context, event Event, entries []listenerEntry) error {
	for _, entry := range entries {
		if entry.async {
			// å¼‚æ­¥ç›‘å¬å™¨æäº¤åˆ°åç¨‹æ± 
			listener := entry.listener
			eventName := event.Name()
			_ = d.pool.Submit(func() {
				if err := listener.Handle(ctx, event); err != nil && !errors.Is(err, ErrStopPropagation) {
					d.logger.ErrorCtx(ctx, "å¼‚æ­¥ç›‘å¬å™¨æ‰§è¡Œå¤±è´¥",
						zap.String("event", eventName),
						zap.Error(err))
				}
			})
			continue
		}

		// åŒæ­¥æ‰§è¡Œ
		if err := entry.listener.Handle(ctx, event); err != nil {
			return err
		}
	}

	return nil
}

// cleanupOnceListeners æ¸…ç†ä¸€æ¬¡æ€§ç›‘å¬å™¨
func (d *dispatcher) cleanupOnceListeners(eventName string, executed []listenerEntry) {
	var onceIDs []uint64
	for _, e := range executed {
		if e.once {
			onceIDs = append(onceIDs, e.id)
		}
	}

	if len(onceIDs) == 0 {
		return
	}

	d.mu.Lock()
	defer d.mu.Unlock()

	entries := d.listeners[eventName]
	filtered := entries[:0]
	for _, e := range entries {
		remove := false
		for _, id := range onceIDs {
			if e.id == id {
				remove = true
				break
			}
		}
		if !remove {
			filtered = append(filtered, e)
		}
	}
	d.listeners[eventName] = filtered
}

// Close å…³é—­åˆ†å‘å™¨
func (d *dispatcher) Close() {
	atomic.StoreInt32(&d.closed, 1)
	if d.pool != nil {
		d.pool.Release()
	}
}

// ListenerCount è·å–æŒ‡å®šäº‹ä»¶çš„ç›‘å¬å™¨æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
func (d *dispatcher) ListenerCount(eventName string) int {
	d.mu.RLock()
	defer d.mu.RUnlock()
	return len(d.listeners[eventName])
}
